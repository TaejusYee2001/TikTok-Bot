{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions for read/write html\n",
    "def read_html(path): \n",
    "    with open(path, 'rb') as f: \n",
    "        return f.read()\n",
    "\n",
    "def write_html(html, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles found: 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Define website url and save html data \\nurl = \\'https://www.reddit.com/r/AmItheAsshole/\\'\\nraw_html_path = \\'data/reddit-html.txt\\'\\nheaders = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"}\\nreq = requests.get(url, headers=headers)\\n\\nwrite_html(req.content, raw_html_path)\\n\\n# Check to see if data was written properly \\nraw_html = read_html(raw_html_path)\\n#print(raw_html)\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure Selenium to use headless mode (optional)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "ua = UserAgent()\n",
    "user_agent = ua.random\n",
    "chrome_options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "# Path to your ChromeDriver\n",
    "chromedriver_path = '/path/to/chromedriver'\n",
    "\n",
    "# Initialize WebDriver\n",
    "service = Service()\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Define the URL\n",
    "url = 'https://www.reddit.com/r/AmItheAsshole/'\n",
    "\n",
    "# Open the URL\n",
    "driver.get(url)\n",
    "\n",
    "# Scroll down to load more articles\n",
    "SCROLL_PAUSE_TIME = random.uniform(2, 5)\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "i = 0\n",
    "while (i < 5):\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait to load the page\n",
    "    time.sleep(SCROLL_PAUSE_TIME + random.uniform(0.5, 1.5))\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "    i += 1\n",
    "    \n",
    "    # Random mouse movement to simulate human behavior\n",
    "    action = ActionChains(driver)\n",
    "    action.move_by_offset(random.randint(0, 100), random.randint(0, 100)).perform()\n",
    "    \n",
    "# Save the page source after loading more articles\n",
    "raw_html_path = 'data/reddit-html.txt'\n",
    "with open(raw_html_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(driver.page_source)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Now you can parse the saved HTML as before\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(raw_html_path, 'r', encoding='utf-8') as file:\n",
    "    raw_html = file.read()\n",
    "\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "articles = soup.find_all('article', class_='w-full m-0')\n",
    "\n",
    "# Print the number of articles found\n",
    "print(f\"Number of articles found: {len(articles)}\")\n",
    "\n",
    "'''\n",
    "# Define website url and save html data \n",
    "url = 'https://www.reddit.com/r/AmItheAsshole/'\n",
    "raw_html_path = 'data/reddit-html.txt'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"}\n",
    "req = requests.get(url, headers=headers)\n",
    "\n",
    "write_html(req.content, raw_html_path)\n",
    "\n",
    "# Check to see if data was written properly \n",
    "raw_html = read_html(raw_html_path)\n",
    "#print(raw_html)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse raw HTML to extract text\n",
    "#soup = BeautifulSoup(req.content, 'html.parser')\n",
    "#articles = soup.find_all('article', class_='w-full m-0') #TODO: Populate articles with more articles (selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.reddit.com/r/AmItheAsshole/comments/1dk24fp/aita_for_cooking_meat_at_night_when_my_upstairs/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dk1oim/aita_for_yelling_at_my_drunk_friend/', 'https://www.reddit.com/r/AmItheAsshole/comments/1diqid8/aita_for_walking_away_from_my_fils_wife_after_she/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dk1ahi/aita_for_changing_my_plans/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djs5r5/aita_for_not_taking_care_of_my_sisters_cat_when/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dk3jwx/aita_for_telling_my_husband_he_sounds_rude_when/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djmncq/aita_for_not_respecting_my_roommates/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djdrp6/aita_for_being_angry_that_my_parents_tell_my/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dk377d/aita_for_calling_out_my_friend_after_she_lied_to/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djszuw/aita_for_asking_my_gma_to_tell_my_mom_that_she/', 'https://www.reddit.com/r/AmItheAsshole/comments/1diwozs/aita_for_telling_my_sister_to_just_not_come_to_my/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dju6q9/aita_for_not_letting_my_mother_sleep_in_my_bed/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djwm2z/aita_for_sleeping_next_to_my_nephew/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djzzfe/aita_not_inclined_to_talk_to_my_old_best_friend/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djp0bv/aita_asked_bff_to_replace_shorts/', 'https://www.reddit.com/r/AmItheAsshole/comments/1diq904/aita_for_being_happy_and_expressing_it_when_i/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dk1y7i/aita_for_expecting_more_for_fathers_day/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djhyl0/wibta_for_continue_to_go_on_a_trip_abroad_without/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djjnze/wibta_for_not_helping_my_bf_buying_things_for_his/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dj8vk1/aita_sister_hidden_pregnancy/', 'https://www.reddit.com/r/AmItheAsshole/comments/1diwure/aita_for_not_giving_my_ex_wife_25_dollars_for_a/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djridj/aita_for_telling_my_friends_theyre_unambitious/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djvb28/aita_for_making_a_point_over_hair/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djxa3k/wibta_if_i_cancel_a_gathering_because_of_my_bil/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djgfkl/aita_for_not_being_happy_about_boyfriends_group/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djzsja/aita_for_not_letting_my_cousin_use_my_hair/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djzjwu/aita_for_berating_my_friend/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dk3jpq/aitah_if_i_let_my_phone_die_when_i_went_to_the_bar/', 'https://www.reddit.com/r/AmItheAsshole/comments/1ditt21/aita_for_telling_my_girlfriends_nephew_he_wasnt/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dk3ggu/aita_for_telling_my_coach_about_what_my_mom_did/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djysad/aita_gas_station_beer/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dji7ly/aita_for_wanting_everyone_to_do_their_own_laundry/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djwysp/aita_for_locking_my_younger_brother_out_of_home/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djtnt8/aita_for_leaving_my_friends_birthday_party_early/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dizlv1/aita_for_not_agreeing_to_go_on_a_family_vacation/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dk0ffz/aita_for_my_dog_barking/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djqbhh/aita_just_wanted_to_watch_movie/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djzetz/aita_help_and_debt_increasing/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dj4g3s/aita_for_not_helping_my_ex_go_to_grad_school_by/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djc1r9/aita_for_asking_my_roommates_to_not_pay_double/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djz78l/aita_for_asking_my_stepdads_cousin_not_to_drink/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djz4sc/aita_for_threatening_to_spoil_the_next_season_of/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djtkb1/aita_for_asking_neighbor_to_limit_drug_use_in/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djykzx/aita_for_always_making_excuses_to_not_hang_out/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djyi96/aita_for_letting_my_mother_know_i_was_expecting/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djyfyr/aita_or_is_my_girlfriend_making_me_not_replying/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djxei0/aita_for_staying_at_home_to_care_for_my_brothers/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djovp3/aita_leaving_my_family_to_focus_on_myself_and_my/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djwrzc/aita_for_currently_doing_nothing_at_my_work/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djct22/aitah_for_not_baking_my_sils_daughter_her_18th/']\n"
     ]
    }
   ],
   "source": [
    "links = []\n",
    "\n",
    "for article in articles: \n",
    "    shreddit_post = article.find('shreddit-post')\n",
    "    links.append(shreddit_post.get('content-href'))\n",
    "\n",
    "with open('data/reddit-links.txt', 'w', encoding='utf-8') as f: \n",
    "    for link in links:\n",
    "        f.write(link + '\\n')\n",
    "        \n",
    "        \n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"}\n",
    "for index, link in enumerate(links): \n",
    "     aita_post_req = requests.get(link, headers=headers)\n",
    "     file_path = f'data/posts-html/aita-post{index}-html.txt'\n",
    "     write_html(aita_post_req.content, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'data/posts-html'\n",
    "items = os.listdir(directory_path)\n",
    "for index, item in enumerate(items): \n",
    "    aita_post_soup = BeautifulSoup(read_html(directory_path + '/' + item), 'html.parser')\n",
    "    post_container = aita_post_soup.find('div', class_='text-neutral-content')\n",
    "    div1 = post_container.find('div')\n",
    "    div2 = div1.find('div')\n",
    "    p_elements = div2.find_all('p')\n",
    "    post_text = '\\n\\n'.join(p.get_text(strip=True) for p in p_elements) # Concatenate text\n",
    "    file_path = f'data/posts/aita-post{index}.txt'\n",
    "    with open(file_path, 'w', encoding='utf-8') as f: \n",
    "        f.write(post_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Get text from post url\\naita_post_soup = BeautifulSoup(aita_post_req.content, 'html.parser')\\npost_container = aita_post_soup.find('div', class_='text-neutral-content')\\ndiv1 = post_container.find('div')\\ndiv2 = div1.find('div')\\np_elements = div2.find_all('p')\\npost_text = '\\n\\n'.join(p.get_text(strip=True) for p in p_elements) # Concatenate text\\n\\nwith open('data/post_text.txt', 'w', encoding='utf-8') as f:\\n        f.write(post_text)\\n          \\nprint(post_text)\\n\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Get text from post url\n",
    "aita_post_soup = BeautifulSoup(aita_post_req.content, 'html.parser')\n",
    "post_container = aita_post_soup.find('div', class_='text-neutral-content')\n",
    "div1 = post_container.find('div')\n",
    "div2 = div1.find('div')\n",
    "p_elements = div2.find_all('p')\n",
    "post_text = '\\n\\n'.join(p.get_text(strip=True) for p in p_elements) # Concatenate text\n",
    "\n",
    "with open('data/post_text.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(post_text)\n",
    "          \n",
    "print(post_text)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# New fetch request on AITA post url\\naita_post_req = requests.get(links[0], headers=headers)\\nwrite_html(aita_post_req.content, 'data/aita-post-html.txt')\\nhtml = read_html('data/aita-post-html.txt')\\nprint(html)\\n\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# New fetch request on AITA post url\n",
    "aita_post_req = requests.get(links[0], headers=headers)\n",
    "write_html(aita_post_req.content, 'data/aita-post-html.txt')\n",
    "html = read_html('data/aita-post-html.txt')\n",
    "print(html)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
