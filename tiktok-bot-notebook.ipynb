{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions for read/write html\n",
    "def read_html(path): \n",
    "    with open(path, 'rb') as f: \n",
    "        return f.read()\n",
    "\n",
    "def write_html(html, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles found: 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Define website url and save html data \\nurl = \\'https://www.reddit.com/r/AmItheAsshole/\\'\\nraw_html_path = \\'data/reddit-html.txt\\'\\nheaders = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"}\\nreq = requests.get(url, headers=headers)\\n\\nwrite_html(req.content, raw_html_path)\\n\\n# Check to see if data was written properly \\nraw_html = read_html(raw_html_path)\\n#print(raw_html)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure Selenium to use headless mode (optional)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "ua = UserAgent()\n",
    "user_agent = ua.random\n",
    "chrome_options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "# Path to your ChromeDriver\n",
    "chromedriver_path = '/path/to/chromedriver'\n",
    "\n",
    "# Initialize WebDriver\n",
    "service = Service()\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Define the URL\n",
    "url = 'https://www.reddit.com/r/AmItheAsshole/'\n",
    "\n",
    "# Open the URL\n",
    "driver.get(url)\n",
    "\n",
    "# Scroll down to load more articles\n",
    "SCROLL_PAUSE_TIME = random.uniform(2, 5)\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "i = 0\n",
    "while (i < 5):\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait to load the page\n",
    "    time.sleep(SCROLL_PAUSE_TIME + random.uniform(0.5, 1.5))\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "    i += 1\n",
    "    \n",
    "    # Random mouse movement to simulate human behavior\n",
    "    action = ActionChains(driver)\n",
    "    action.move_by_offset(random.randint(0, 100), random.randint(0, 100)).perform()\n",
    "    \n",
    "# Save the page source after loading more articles\n",
    "raw_html_path = 'data/reddit-html.txt'\n",
    "with open(raw_html_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(driver.page_source)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Now you can parse the saved HTML as before\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(raw_html_path, 'r', encoding='utf-8') as file:\n",
    "    raw_html = file.read()\n",
    "\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "articles = soup.find_all('article', class_='w-full m-0')\n",
    "\n",
    "# Print the number of articles found\n",
    "print(f\"Number of articles found: {len(articles)}\")\n",
    "\n",
    "'''\n",
    "# Define website url and save html data \n",
    "url = 'https://www.reddit.com/r/AmItheAsshole/'\n",
    "raw_html_path = 'data/reddit-html.txt'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"}\n",
    "req = requests.get(url, headers=headers)\n",
    "\n",
    "write_html(req.content, raw_html_path)\n",
    "\n",
    "# Check to see if data was written properly \n",
    "raw_html = read_html(raw_html_path)\n",
    "#print(raw_html)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse raw HTML to extract text\n",
    "#soup = BeautifulSoup(req.content, 'html.parser')\n",
    "#articles = soup.find_all('article', class_='w-full m-0') #TODO: Populate articles with more articles (selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.reddit.com/r/AmItheAsshole/comments/1dk8j6a/aita_for_telling_my_dad_he_probably_couldnt_do_a/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkkuhn/aita_for_not_wanting_to_go_to_the_pool_when_my/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djpnyk/aita_for_tutoring_my_sisters_bully_and_telling/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkh8lg/wibta_if_i_dont_bother_going_to_my_birthday_party/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkgtls/aita_for_trying_to_save_my_moms_house/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkei9j/aita_for_not_changing_sleeping_habits/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djtpix/aita_for_agreeing_to_walk_my_nephew_down_the_aisle/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkjh4p/aita_for_excluding_my_boyfriend_from_my/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dk53hq/aita_having_applied_to_an_university_in_another/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dj3if5/aita_for_telling_a_parent_that_her_autistic_child/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkex94/aita_for_making_my_sisters_friend_leave/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djp7lo/aita_for_threatening_to_arrest_my_stepfather/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dk18iv/aita_for_not_giving_my_sibling_money/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkmdte/aita_for_not_explaining_why_i_didnt_invite_a/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkma26/aita_for_having_my_2_month_old_wear_white_to_a/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dk3f9c/aita_for_not_inviting_my_grandparents_to_the/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dklvdg/aita_for_not_paying_my_roomies_rent/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dj9cu5/aita_for_making_my_husbands_wife_uncomfortable_at/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkljjc/aita_for_calling_my_dad_out_on_his_diabetes/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkfn99/wibta_if_i_told_my_parents_i_hate_my_major/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djzmpw/aita_cat_got_locked_in_the_bedroom_and_my_partner/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dk2mdv/aita_for_exposing_my_dad/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkkclo/aita_for_surprising_my_gf_after_her_testing/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djvb28/aita_for_making_a_point_over_hair/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dk31tw/wibta_if_i_dont_cancel_preexisting_work_to_attend/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkk8ta/aita_hurt_over_parents_being_unfair_with_money/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkep90/aita_for_telling_my_mom_i_like_to_hang_out_with/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dk6c6s/aita_this_is_a_8_year_long_debateargument/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkk357/wibta_for_not_going_to_the_gas_station/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dk2xgf/aita_i_dont_want_my_mom_or_grandma_at_my/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkbjgk/aita_for_leaving_my_graduation_party_afert_being/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkk1i6/aita_for_resenting_my_husband_for_taking_on_a/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkk0de/aita_for_telling_my_sibling_to_not_have_sex/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djmpmb/wibta_for_taking_away_my_son_from_his_family_and/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkjvkm/aita_for_refusing_to_cuddle_with_my_husband/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dke1mf/aita_for_dropping_a_friend/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkjg99/aita_for_telling_my_sister_to_look_at_herself_her/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djqjum/aita_for_wanting_privacy/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djx1i7/aita_for_telling_my_roommate_that_she_doesnt/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dk5yy5/aita_if_i_say_no_to_my_partner_hanging_out_with/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djsbj9/wibta_for_setting_up_a_safe_in_my_homes_shared/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dki3a5/aita_am_i_the_a_for_not_reading_my_friend_book/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dk6ave/aita_for_taking_time_to_myself_on_an_evening/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkhmaa/aita_for_not_letting_my_brother_buy_a_higher_end/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkhkm3/aita_for_not_having_an_opinion/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkgmjl/aita_for_ghosting_my_bf_because_he_owes_me_money/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dk6gas/aita_for_yelling_at_my_stepdads_son/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkgccj/aita_for_switching_plans_with_my_bf_twice_in_a/', 'https://www.reddit.com/r/AmItheAsshole/comments/1djsy24/aita_for_disagreeing_with_my_partner_when_his/', 'https://www.reddit.com/r/AmItheAsshole/comments/1dkaq4l/aita_for_leaving_some_work_to_my_colleague_while/']\n"
     ]
    }
   ],
   "source": [
    "links = []\n",
    "\n",
    "for article in articles: \n",
    "    shreddit_post = article.find('shreddit-post')\n",
    "    links.append(shreddit_post.get('content-href'))\n",
    "\n",
    "with open('data/reddit-links.txt', 'w', encoding='utf-8') as f: \n",
    "    for link in links:\n",
    "        f.write(link + '\\n')\n",
    "        \n",
    "        \n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"}\n",
    "for index, link in enumerate(links): \n",
    "     aita_post_req = requests.get(link, headers=headers)\n",
    "     file_path = f'data/posts-html/aita-post{index}-html.txt'\n",
    "     write_html(aita_post_req.content, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'data/posts-html'\n",
    "items = os.listdir(directory_path)\n",
    "for index, item in enumerate(items): \n",
    "    aita_post_soup = BeautifulSoup(read_html(directory_path + '/' + item), 'html.parser')\n",
    "    post_container = aita_post_soup.find('div', class_='text-neutral-content')\n",
    "    div1 = post_container.find('div')\n",
    "    div2 = div1.find('div')\n",
    "    p_elements = div2.find_all('p')\n",
    "    post_text = '\\n\\n'.join(p.get_text(strip=True) for p in p_elements) # Concatenate text\n",
    "    file_path = f'data/posts/aita-post{index}.txt'\n",
    "    with open(file_path, 'w', encoding='utf-8') as f: \n",
    "        f.write(post_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
