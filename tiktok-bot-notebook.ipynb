{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions for read/write html\n",
    "def read_html(path): \n",
    "    with open(path, 'rb') as f: \n",
    "        return f.read()\n",
    "\n",
    "def write_html(html, path):\n",
    "    directory = os.path.dirname(path)\n",
    "    if not os.path.exists(directory): \n",
    "        os.makedirs(directory)\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make website cache many posts by scrolling and save html content afterwards\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "ua = UserAgent()\n",
    "user_agent = ua.random\n",
    "chrome_options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "# Initialize WebDriver\n",
    "service = Service()\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "url = 'https://www.reddit.com/r/AmItheAsshole/'\n",
    "driver.get(url)\n",
    "\n",
    "SCROLL_PAUSE_TIME = random.uniform(2, 5)\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "num_scrolls = 0\n",
    "while (num_scrolls < 5):\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    time.sleep(SCROLL_PAUSE_TIME + random.uniform(0.5, 1.5))\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "    num_scrolls += 1\n",
    "    \n",
    "    # Random mouse movement to simulate human behavior\n",
    "    action = ActionChains(driver)\n",
    "    action.move_by_offset(random.randint(0, 100), random.randint(0, 100)).perform()\n",
    "    \n",
    "raw_html_path = 'data/aita-reddit-html.txt'\n",
    "directory = os.path.dirname(raw_html_path)\n",
    "\n",
    "if not os.path.exists(directory): \n",
    "    os.makedirs(directory)\n",
    "    \n",
    "with open(raw_html_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(driver.page_source)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles found: 50\n"
     ]
    }
   ],
   "source": [
    "# Parse webpage HTML and get links to each article\n",
    "with open(raw_html_path, 'r', encoding='utf-8') as file:\n",
    "    raw_html = file.read()\n",
    "\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "articles = soup.find_all('article', class_='w-full m-0')\n",
    "\n",
    "print(f\"Number of articles found: {len(articles)}\")\n",
    "\n",
    "links = []\n",
    "for article in articles: \n",
    "    shreddit_post = article.find('shreddit-post')\n",
    "    links.append(shreddit_post.get('content-href'))\n",
    "\n",
    "with open('data/reddit-links.txt', 'w', encoding='utf-8') as f: \n",
    "    for link in links:\n",
    "        f.write(link + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform new fetch on each post link\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"}\n",
    "for index, link in enumerate(links): \n",
    "     aita_post_req = requests.get(link, headers=headers)\n",
    "     file_path = f'data/posts-html/aita-post{index}-html.txt'\n",
    "     write_html(aita_post_req.content, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text from HTML served from the post link\n",
    "posts_html_path = 'data/posts-html'\n",
    "posts_path = 'data/posts'\n",
    "\n",
    "if not os.path.exists(posts_path): \n",
    "    os.makedirs(posts_path)\n",
    "\n",
    "items = os.listdir(posts_html_path)\n",
    "for index, item in enumerate(items): \n",
    "    aita_post_soup = BeautifulSoup(read_html(posts_html_path + '/' + item), 'html.parser')\n",
    "    post_container = aita_post_soup.find('div', class_='text-neutral-content')\n",
    "    div1 = post_container.find('div')\n",
    "    div2 = div1.find('div')\n",
    "    p_elements = div2.find_all('p')\n",
    "    post_text = '\\n\\n'.join(p.get_text(strip=True) for p in p_elements) # Concatenate text\n",
    "    file_path = os.path.join(posts_path, f'aita-post{index}.txt')\n",
    "    with open(file_path, 'w', encoding='utf-8') as f: \n",
    "        f.write(post_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text To Speech"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
