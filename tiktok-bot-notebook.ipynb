{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "import time\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions for read/write html\n",
    "def read_html(path): \n",
    "    with open(path, 'rb') as f: \n",
    "        return f.read()\n",
    "\n",
    "def write_html(html, path):\n",
    "    directory = os.path.dirname(path)\n",
    "    if not os.path.exists(directory): \n",
    "        os.makedirs(directory)\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make website cache many posts by scrolling and save html content afterwards\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "ua = UserAgent()\n",
    "user_agent = ua.random\n",
    "chrome_options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "# Initialize WebDriver\n",
    "service = Service()\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "url = 'https://www.reddit.com/r/AmItheAsshole/'\n",
    "driver.get(url)\n",
    "\n",
    "SCROLL_PAUSE_TIME = random.uniform(2, 5)\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "num_scrolls = 0\n",
    "while (num_scrolls < 5):\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    time.sleep(SCROLL_PAUSE_TIME + random.uniform(0.5, 1.5))\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "    num_scrolls += 1\n",
    "    \n",
    "    # Random mouse movement to simulate human behavior\n",
    "    action = ActionChains(driver)\n",
    "    action.move_by_offset(random.randint(0, 100), random.randint(0, 100)).perform()\n",
    "    \n",
    "raw_html_path = 'data/aita-reddit-html.txt'\n",
    "directory = os.path.dirname(raw_html_path)\n",
    "\n",
    "if not os.path.exists(directory): \n",
    "    os.makedirs(directory)\n",
    "    \n",
    "with open(raw_html_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(driver.page_source)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse webpage HTML and get links to each article\n",
    "with open(raw_html_path, 'r', encoding='utf-8') as file:\n",
    "    raw_html = file.read()\n",
    "\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "articles = soup.find_all('article', class_='w-full m-0')\n",
    "\n",
    "print(f\"Number of articles found: {len(articles)}\")\n",
    "\n",
    "links = []\n",
    "for article in articles: \n",
    "    shreddit_post = article.find('shreddit-post')\n",
    "    links.append(shreddit_post.get('content-href'))\n",
    "\n",
    "with open('data/reddit-links.txt', 'w', encoding='utf-8') as f: \n",
    "    for link in links:\n",
    "        f.write(link + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform new fetch on each post link\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"}\n",
    "for index, link in enumerate(links): \n",
    "     aita_post_req = requests.get(link, headers=headers)\n",
    "     file_path = f'data/posts-html/aita-post{index}-html.txt'\n",
    "     write_html(aita_post_req.content, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text from HTML served from the post link\n",
    "posts_html_path = 'data/posts-html'\n",
    "posts_path = 'data/posts'\n",
    "allowed_chars = set(string.printable)\n",
    "\n",
    "if not os.path.exists(posts_path): \n",
    "    os.makedirs(posts_path)\n",
    "\n",
    "items = os.listdir(posts_html_path)\n",
    "for index, item in enumerate(items): \n",
    "    aita_post_soup = BeautifulSoup(read_html(posts_html_path + '/' + item), 'html.parser')\n",
    "    post_container = aita_post_soup.find('div', class_='text-neutral-content')\n",
    "    div1 = post_container.find('div')\n",
    "    div2 = div1.find('div')\n",
    "    p_elements = div2.find_all('p')\n",
    "    post_text = '\\n\\n'.join(p.get_text(strip=True) for p in p_elements) # Concatenate text\n",
    "    file_path = os.path.join(posts_path, f'aita-post{index}.txt')\n",
    "    with open(file_path, 'w', encoding='utf-8') as f: \n",
    "        post_text = ''.join(filter(lambda x: x in allowed_chars, post_text))\n",
    "        f.write(post_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text To Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create audio file from text\n",
    "def textToSpeech(text, output_file): \n",
    "    tts = gTTS(text=text, lang='en', slow=False)\n",
    "    tts.save(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text from data/posts directory and save audio files in audio directory\n",
    "audio_dir = 'output/audio'\n",
    "if not os.path.exists(audio_dir): \n",
    "    os.makedirs(audio_dir)\n",
    "    \n",
    "for index, filename in enumerate(os.listdir(posts_path)): \n",
    "    path = os.path.join(posts_path, filename)\n",
    "    with open(path, 'r') as file: \n",
    "        text = file.read()\n",
    "        \n",
    "    textToSpeech(text, os.path.join(audio_dir, f'post-audio{index}.mp3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternate tts package 11 labs\n",
    "\n",
    "\n",
    "#imports\n",
    "import uuid\n",
    "from elevenlabs import VoiceSettings\n",
    "from elevenlabs.client import ElevenLabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup from elevenlabs API documentation\n",
    "\n",
    "\n",
    "os.environ['ELEVENLABS_API_KEY'] = 'sk_e8258a49a90cc1d679bb949d514810e23a0e7e3c288b3cb8'\n",
    "ELEVENLABS_API_KEY = os.getenv(\"ELEVENLABS_API_KEY\")\n",
    "client = ElevenLabs(api_key=ELEVENLABS_API_KEY,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tts function to replace gtts: with customization of voice, speed etc\n",
    "#write to specified file as well jsut like the gtts one\n",
    "\n",
    "\n",
    "def text_to_speech_file(text: str, output_file: str) -> str:\n",
    "    response = client.text_to_speech.convert(\n",
    "        voice_id=\"pNInz6obpgDQGcFmaJgB\",  # Adam pre-made voice\n",
    "        optimize_streaming_latency=\"0\",\n",
    "        output_format=\"mp3_22050_32\",\n",
    "        text=text,\n",
    "        model_id=\"eleven_turbo_v2\",  # Use the turbo model for low latency, for other languages use the `eleven_multilingual_v2`\n",
    "        voice_settings=VoiceSettings(\n",
    "            stability=0.0,\n",
    "            similarity_boost=1.0,\n",
    "            style=0.0,\n",
    "            use_speaker_boost=True,\n",
    "        ),\n",
    "    )\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        for chunk in response:\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "    print(f\"{output_file}: A new audio file was saved successfully!\")\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to same audio dirs as previous fn\n",
    "audio_dir = 'output/audio'\n",
    "if not os.path.exists(audio_dir):\n",
    "    os.makedirs(audio_dir)\n",
    "\n",
    "\n",
    "for index, filename in enumerate(os.listdir(posts_path)):\n",
    "    path = os.path.join(posts_path, filename)\n",
    "    with open(path, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "\n",
    "    output_file = os.path.join(audio_dir, f'post-audio{index}.mp3')\n",
    "    text_to_speech_file(text, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Subtitle Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import whisper\n",
    "import ffmpeg\n",
    "import subprocess\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get aligned subtitles from audio file\n",
    "model = whisper.load_model('base')\n",
    "audio_file_path = 'output/audio/post-audio9.mp3'\n",
    "\n",
    "result = model.transcribe(audio_file_path)\n",
    "print(result[\"segments\"])\n",
    "for segment in result['segments']: \n",
    "    print(segment)\n",
    "    \n",
    "#options = whisper.DecodingOptions(fp16=False)\n",
    "#decoded_result = whisper.decode(model, result['audio'], options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_path = 'output/audio/post-audio9.mp3'\n",
    "video_file_path = 'output/video/post-video9.mp4'\n",
    "\n",
    "ffmpeg_command = [\n",
    "    'ffmpeg',\n",
    "    '-f', 'lavfi',\n",
    "    '-i', 'color=size=1280x720:duration=30:rate=30:color=black',\n",
    "    '-i', audio_file_path,\n",
    "    '-c:v', 'libx264',\n",
    "    '-tune', 'stillimage',\n",
    "    '-c:a', 'aac',\n",
    "    '-b:a', '192k',\n",
    "    '-pix_fmt', 'yuv420p',\n",
    "    '-shortest',\n",
    "    video_file_path\n",
    "]\n",
    "\n",
    "subprocess.run(ffmpeg_command, check=True)\n",
    "\n",
    "print(\"saved video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = 'output/video/post-video9.mp4'\n",
    "audio_output = 'output/audio/extracted-audio.mp3'\n",
    "subtitles_file = 'output/subtitles/subtitles.srt'  # Assume this file is generated\n",
    "output_video_file = 'output/video/post-video9-subtitled.mp4'\n",
    "\n",
    "# Ensure the output directories exist\n",
    "os.makedirs(os.path.dirname(audio_output), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(subtitles_file), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(output_video_file), exist_ok=True)\n",
    "\n",
    "# Extract audio from the video\n",
    "extract_audio_command = [\n",
    "    'ffmpeg',\n",
    "    '-i', video_file,\n",
    "    '-q:a', '0',\n",
    "    '-map', 'a',\n",
    "    audio_output\n",
    "]\n",
    "\n",
    "# Run the FFmpeg commands\n",
    "try:\n",
    "    # Extract audio\n",
    "    subprocess.run(extract_audio_command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    print(f\"Audio extracted to {audio_output}\")\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(f\"Command output: {e.output}\")\n",
    "    print(f\"Stderr: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_output = 'output/audio/extracted-audio.mp3'\n",
    "subtitles_file = 'output/subtitles/subtitles.srt'\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary())\n",
    "\n",
    "model = whisper.load_model('base')\n",
    "\n",
    "# Transcribe the audio file\n",
    "result = model.transcribe(word_timestamps=True, audio=audio_output)\n",
    "\n",
    "# Function to convert seconds to SRT time format\n",
    "def seconds_to_srt_time(seconds):\n",
    "    millisec = int((seconds - int(seconds)) * 1000)\n",
    "    time_str = f\"{int(seconds // 3600):02}:{int((seconds % 3600) // 60):02}:{int(seconds % 60):02},{millisec:03}\"\n",
    "    return time_str\n",
    "\n",
    "# Create the SRT content\n",
    "srt_content = \"\"\n",
    "counter = 1\n",
    "\n",
    "for segment in result['segments']:\n",
    "    for word in segment['words']: \n",
    "        start_time = seconds_to_srt_time(segment['start'])\n",
    "        end_time = seconds_to_srt_time(segment['end'])\n",
    "        text = word['word']\n",
    "        srt_content += f\"{counter}\\n{start_time} --> {end_time}\\n{text.strip()}\\n\\n\"\n",
    "        counter += 1\n",
    "\n",
    "# Write the SRT content to a file\n",
    "with open(subtitles_file, 'w', encoding='utf-8') as srt_file:\n",
    "    srt_file.write(srt_content)\n",
    "\n",
    "print(f\"Subtitles have been saved as {subtitles_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = 'output/video/post-video9.mp4'\n",
    "output_video_file = 'output/video/post-video9-subtitled.mp4'\n",
    "subtitles_file = 'output/subtitles/subtitles.srt'\n",
    "\n",
    "# Add subtitles to the video\n",
    "add_subtitles_command = [\n",
    "    'ffmpeg',\n",
    "    '-i', video_file,\n",
    "    '-vf', f\"subtitles={subtitles_file}\",\n",
    "    '-c:a', 'copy',\n",
    "    output_video_file\n",
    "]\n",
    "\n",
    "# Run the FFmpeg command to add subtitles\n",
    "try:\n",
    "    add_subtitles_result = subprocess.run(add_subtitles_command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    print(f\"Subtitled video created at {output_video_file}\")\n",
    "    print(add_subtitles_result.stdout)\n",
    "    print(add_subtitles_result.stderr)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"An error occurred while adding subtitles: {e}\")\n",
    "    print(f\"Stderr: {e.stderr}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
